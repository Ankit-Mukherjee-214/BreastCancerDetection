{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAST CANCER PREDICTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary packages since we need to use it in the code\n",
    "On the below code we have to load the dataset so that we can get the right kind of data. After loading of the dataset the first five rows are printed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path= \"home/ankit/Breast_Cancer_Prediction_Model/Breast-Cancer-Prediction-Model-using-Machine-Learning/breast cancer.csv\"\n",
    "breast = pd.read_csv(file_path)\n",
    "breast.head ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple understanding of the presented table will be below : \n",
    "\n",
    "_id = patient identifier which should be unique_\n",
    "\n",
    "**diagnosis = which shows what kind of prediction we are expecting**\n",
    "\n",
    "Apart from this all the other columns will turn input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above code shows what kind of diagnosis we are expecting. \n",
    "\n",
    "If 'B' comes it is benign and if 'M' comes it is malignant. \n",
    "\n",
    "So we are having only two types of outputs over here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast.duplicated().sum()\n",
    "breast.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this shows there is no duplicated data in the dataset. \n",
    "\n",
    "The above three commands analyze what type of dataset we are working on over here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast.drop(columns=['Unnamed: 32'], axis=1, inplace=True)\n",
    "# drop the unnecessary columns since it will create noise in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we used the describe function to check the mathematical analysis of the dataset which we are using. It is used for that purpose \n",
    "\n",
    "### Encoding the Target Variable \n",
    "\n",
    "This is required because the output variable B or M, that needs to be changed to binary 0 or 1, so that the machine can understand it. \n",
    "\n",
    "The below package is required for the encoding of the output variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# without using sklearn we can also manually map the output data \n",
    "\n",
    "breast['diagnosis'] = breast['diagnosis'].map({'M' : 1 , 'B' : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast\n",
    "breast['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data into Training Sets and Testing sets \n",
    "\n",
    "Here, we will be filtering the data as inputs and outputs. There will be 31 inputs and the rest would be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= breast.drop(['diagnosis'], axis=1)\n",
    "# apart from the diagnosis column all the others are output of X \n",
    "Y= breast['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to split the data into training and testing sets. This will be done by sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# used for data splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above code defines the data is split at 20% which means, 20% of the data is split to training set and the remaining data is split to test set\n",
    "\n",
    "## Feature Scaling \n",
    "\n",
    "The values are having a lot of deviation. To standardize this, we are introducing feature scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "# this package is used for feature scaling \n",
    "sc = StandardScaler ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.fit(X_train , X_test)\n",
    "X_train = sc.transform (X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using StandardScaler of sklearn package to fit and transform the input data for both the testing and the training sets. \n",
    "\n",
    "## Training the Model \n",
    "\n",
    "We will be using logistic regression and use the training sets and then try to predict the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression()\n",
    "\n",
    "# imported the package and created an object to use it in the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.fit(X_train,Y_train)\n",
    "y_pred = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After prediction we need to see how much accurately it is predicting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = ()\n",
    "np_df = np.asarray (input_user)\n",
    "prediction = lg.predict(np_df.reshape(1,-1))\n",
    "\n",
    "if prediction[0] == 1 : \n",
    "    print (\"Malignant tumour. Chances of having breast cancer. Take care of yourself\")\n",
    "else : \n",
    "    print (\"Benign tumour. No chances of cancer. Please enjoy your life!!! \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So before the prediction we are using the numpy package to change the text to array and reshape it as 1 element array. This is important otherwise we cannot use it in the right way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough Input if required \n",
    "\n",
    "Use X_train tables to get input for testing cases. You can use it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration of this model to Website \n",
    "\n",
    "This is done using a pickle library which is required to be imported. Once used this library we will be using a pickle file and keep this model as write binary mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(lg,open('breast_cancer_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_env)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
